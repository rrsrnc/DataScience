{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm7Qf/mSbJdyfxMC5sH0dt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrsrnc/DataScience/blob/main/week6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "1.   Create a dictionary whose keys correspond to unique words in a given file and whose values correspond to the number of times each word appears in the file.\n",
        "2.   To an output file, write the 10 most frequent words in separate lines.\n",
        "3.   To an output file, write the 5 most frequenct 2-grams in separate lines."
      ],
      "metadata": {
        "id": "wR-N0Vx7-ELD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 1"
      ],
      "metadata": {
        "id": "82vqUo3_-G6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRjY2ZGN19QJ",
        "outputId": "c4286559-6c27-4bc8-8a96-eb71868db462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('of', 15), ('the', 13), ('in', 11), ('and', 7), ('love', 6), ('you', 6), ('forever', 6), ('life', 5), ('i', 4), ('after', 4), ('age', 4), ('have', 3), ('that', 3), ('your', 3), ('its', 3), ('on', 3), ('past', 3), ('universal', 3), ('numberless', 2), ('forms', 2), ('heart', 2), ('has', 2), ('songs', 2), ('as', 2), ('a', 2), ('old', 2), ('end', 2), ('time', 2), ('is', 2), ('at', 2), ('one', 2), ('same', 2), ('renew', 2), ('it', 2), ('all', 2), ('unending', 1), ('by', 1), ('rabindranath', 1), ('tagore', 1), ('seem', 1), ('to', 1), ('loved', 1), ('times', 1), ('my', 1), ('spellbound', 1), ('made', 1), ('remade', 1), ('necklace', 1), ('take', 1), ('gift', 1), ('wear', 1), ('round', 1), ('neck', 1), ('many', 1), ('whenever', 1), ('hear', 1), ('chronicles', 1), ('age-old', 1), ('pain', 1), ('ancient', 1), ('tale', 1), ('being', 1), ('apart', 1), ('or', 1), ('together', 1), ('stare', 1), ('into', 1), ('emerge', 1), ('clad', 1), ('light', 1), ('pole-star', 1), ('piercing', 1), ('darkness', 1), ('become', 1), ('an', 1), ('image', 1), ('what', 1), ('remembered', 1), ('floated', 1), ('here', 1), ('stream', 1), ('brings', 1), ('from', 1), ('fount', 1), ('for', 1), ('another', 1), ('we', 1), ('played', 1), ('along', 1), ('side', 1), ('millions', 1), ('lovers', 1), ('shared', 1), ('shy', 1), ('sweetness', 1), ('meeting', 1), ('distressful', 1), ('tears', 1), ('farewell-', 1), ('but', 1), ('shapes', 1), ('today', 1), ('heaped', 1), ('feet', 1), ('found', 1), ('man’s', 1), ('days', 1), ('both', 1), ('joy', 1), ('sorrow', 1), ('memories', 1), ('loves', 1), ('merging', 1), ('with', 1), ('this', 1), ('ours', 1), ('every', 1), ('poet', 1)]\n"
          ]
        }
      ],
      "source": [
        "with open(\"poem.txt\",\"r\") as f:\n",
        "  outlines=f.readlines()\n",
        "text=\"\"\n",
        "charToReplace=[',','.','\\n','–',\":\"]\n",
        "for i in range(len(outlines)):\n",
        "  outlines[i]=outlines[i].lower()\n",
        "  for char in charToReplace:\n",
        "    outlines[i]=outlines[i].replace(char,\" \")\n",
        "  text+=outlines[i]\n",
        "text=text.split()\n",
        "data=counter(text)\n",
        "data=sorted(data.items(),key=lambda item:item[1],reverse=True)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def counter(text):\n",
        "  data={}\n",
        "  for word in text:\n",
        "    if word in data:\n",
        "      data[word]+=1\n",
        "    else:\n",
        "      data[word]=1\n",
        "  return data"
      ],
      "metadata": {
        "id": "C7QQaQ46MCKe"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 2\n"
      ],
      "metadata": {
        "id": "DDw4vFMw-DE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"frequentWords.txt\",\"w\") as nf:\n",
        "  for i in range(11):\n",
        "    nf.writelines(data[i][0]+\":\"+str(data[i][1])+\"\\n\")"
      ],
      "metadata": {
        "id": "YhOjDXhG2PGn"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 3"
      ],
      "metadata": {
        "id": "gAQ_-cQFOufS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twoGrams=zip(text,text[1:])\n",
        "# print(*twoGrams)\n",
        "\n",
        "twoGramsFreq=counter(twoGrams)\n",
        "twoGramsFreqsorted=sorted(twoGramsFreq.items(),key=lambda item:item[1],reverse=True)\n",
        "# print(twoGramsFreqsorted)\n",
        "\n",
        "with open(\"common_two_grams.txt\", \"w\") as f_out:\n",
        "    for item in twoGramsFreqsorted[-5:]:\n",
        "        # print(item[0][0] + \" \" + item[0][1] + \"\\n\")\n",
        "        f_out.write(item[0][0] + \" \" + item[0][1] + \"\\n\")"
      ],
      "metadata": {
        "id": "FNEohuoCOuEq"
      },
      "execution_count": 135,
      "outputs": []
    }
  ]
}